{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One_Pass version: 0.7.2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os \n",
    "import requests\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "\n",
    "# importing the TDigest package\n",
    "from crick import TDigest\n",
    "\n",
    "#Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.patches as patches\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from one_pass import __version__ as one_pass_version\n",
    "from one_pass.opa import Opa\n",
    "from utils import read_or_download_dataset\n",
    "\n",
    "print(f\"One_Pass version: {one_pass_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and extract wind speed. \n",
    "\n",
    "The wind speed has been calculated using the square root of the sum of the two horizontal wind components from the IFS model, run as part of the nextGEMS cycle 3 runs.\n",
    "\n",
    "Showing hourly data over December 2020.\n",
    "\n",
    "We have re-sampled the data down to r100 for usabiltiy: this allows us to demonstrate the streaming algorithms by loading all the data in memory and then looping through.\n",
    "\n",
    "In operational mode this would done via an automated workflow manager or similar that would pass higher resolution data to the algorithms chunk by chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if not present\n",
    "\n",
    "URLS = (\n",
    "    (\"ws_2020.nc\", \"https://zenodo.org/records/12533197/files/wind_speed_dec_2020_IFS_tco2559_ng5_cycle3_r100.nc?download=1\"),\n",
    "    (\"ws_2020_texas.nc\", \"https://zenodo.org/records/12533197/files/wind_speed_dec_texas_IFS_tco2559_ng5_cycle3_r010.nc?download=1\"),\n",
    "    (\"ws_2020_uk.nc\", \"https://zenodo.org/records/12533197/files/wind_speed_dec_uk_IFS_tco2559_ng5_cycle3_r010.nc?download=1\"),\n",
    ")\n",
    "\n",
    "ws = read_or_download_dataset(*URLS[0])[\"wind_speed_rms_u10_v10\"]\n",
    "ws_texas = read_or_download_dataset(*URLS[1]).to_dataarray().data\n",
    "ws_uk = read_or_download_dataset(*URLS[2]).to_dataarray().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the functions 'init_digests' and 'update_digests' that creates digests for the streamed data and adds the new data chunks to the digests. We use a weight of 4 for each data chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0d233e5a614b56a28fd4a763b02d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract data array\n",
    "\n",
    "step = 4\n",
    "n = 0\n",
    "\n",
    "# inside the package this calculated based on the time stamp of the incoming data and the\n",
    "# time step of the data, however here just setting it for convienece\n",
    "c = len(ws.time)\n",
    "\n",
    "opa_stat = Opa({\n",
    "    \"variable\": \"wind_speed_rms_u10_v10\",\n",
    "    \"stat\": \"percentile\",\n",
    "    \"percentile_list\": [],\n",
    "    \"stat_freq\": \"monthly\",\n",
    "    \"output_freq\": \"monthly\",\n",
    "    \"time_step\": 60,  # minutes\n",
    "    \"checkpoint\": False,\n",
    "    \"save\": False,\n",
    "})\n",
    "\n",
    "# simulating the streamed data\n",
    "for hour in tqdm(range(0, c, step)):\n",
    "\n",
    "    incoming_data_chunk = ws.isel(time = slice(hour, hour+step))\n",
    "    res = opa_stat.compute(incoming_data_chunk)\n",
    "\n",
    "size_data_source_tail = np.size(incoming_data_chunk.tail(time=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to calculate the estimated value of each percentile from using the t-digest comapred to the estimate from using numpy. \n",
    "\n",
    "Here we calculate the numpy estimate for every percentile value over the spatial grid (we flatten the array to match the shape of the digest list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a9ff25c6af4fe289bcf11db799c2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list of quantiles we want to loop thorugh\n",
    "quantiles = res.percentile.values * 100\n",
    "# empty array that will story all the numpy percentile estimates for every grid cell\n",
    "percen_np = np.zeros([len(quantiles), size_data_source_tail])\n",
    "# flattening the array over the spatial grid\n",
    "ws_flat = np.reshape(ws.values, [len(ws.time), size_data_source_tail])\n",
    "# going through every quantile \n",
    "for index, quantile in tqdm(enumerate(quantiles), total=len(quantiles)):\n",
    "    percen_np[index, :] = np.percentile(ws_flat, axis = 0, q = quantile, method = 'linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the t-dgiest estimate for every percentile value over the spatial grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the numpy estimate against the t-dgiest estimate and provide the error as a percentage of the numpy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percen_opa = res[\"wind_speed_rms_u10_v10\"].values[0, :, :, :].reshape((100, -1))\n",
    "time_dim, lat_size, lon_size = np.shape(incoming_data_chunk)\n",
    "\n",
    "per_error = ((percen_opa - percen_np)/percen_np) * 100        \n",
    "\n",
    "# averaging the error across all the percentiles\n",
    "average_error = np.mean(abs(per_error), axis = 0)\n",
    "\n",
    "# reshape array back to lat lon\n",
    "total_per_error = np.reshape(average_error, [1, lat_size, lon_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huge\n",
    "\n",
    "total_per_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the two time series for the Moray East wind farm in the North Sea and the Rosoe wind farm in Texas, North America. These time series were extracted from the native resolution data to be more accurate with the location. If you want to change the location of these points to examine different locations, uncomment the code below (ws_uk or ws_texas) |to select dat from the downsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_closest_value(sequence, value):\n",
    "  array = np.asarray(sequence)\n",
    "  index = (np.abs(array - value)).argmin()\n",
    "  return array[index]\n",
    "\n",
    "def get_index_lat_lon(ds, lat, lon):\n",
    "\n",
    "    target_lat_new = _get_closest_value(ws.lat.values, lat)\n",
    "    target_lon_new = _get_closest_value(ws.lon.values, lon)\n",
    "\n",
    "    closest_lat = _get_closest_value(ds.lat.values, lat)\n",
    "    closest_lon = _get_closest_value(ds.lon.values, lon)\n",
    "\n",
    "    index_lat = np.where(ds.lat.values == closest_lat)[0][0]\n",
    "    index_lon = np.where(ds.lon.values == closest_lon)[0][0]\n",
    "\n",
    "    return index_lat, index_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we match the co-ordinates of the two wind farm locations to their closest location on the overall wind speed dataset to allow for plotting. \n",
    "\n",
    "To use data from the downsampled data set, uncomment the ws_uk line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target lat and lon for the Moray East wind farm\n",
    "uk_latlon = (58.25, 2.75)\n",
    "uk_latlon_index = get_index_lat_lon(ws, *uk_latlon)\n",
    "# uk_time_series = ws[:, uk_latlon_index[0], uk_latlon_index[1]]\n",
    "\n",
    "# target lat and lon for the Roscoe wind farm\n",
    "texas_latlon = (32.35, 360 - 100.55)\n",
    "texas_latlon_index = get_index_lat_lon(ws, *texas_latlon)\n",
    "# texas_time_series = ws[:, texas_latlon_index[0], texas_latlon_index[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using these two specific locations to evaluate the effect of the compression parameter on the t-digest estimate. As the 'init_digests' and 'update_digests' functions use a compression factor of 60, here we are intialsing and updating the digests explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_spacing = 2\n",
    "\n",
    "# min and max compression factors to span across\n",
    "max_comp = 140\n",
    "min_comp = 20\n",
    "spacing = int((max_comp - min_comp)/compression_spacing)+1\n",
    "compressions = np.linspace(min_comp, max_comp, spacing)\n",
    "num_compressions = len(compressions)\n",
    "\n",
    "tdigest_list_uk = []\n",
    "tdigest_list_texas = []\n",
    "\n",
    "for compression in compressions:\n",
    "    tdigest_uk = TDigest(compression=compression)\n",
    "    tdigest_texas = TDigest(compression=compression)\n",
    "\n",
    "    # simulating streaming by adding data to the digests incrementally\n",
    "    for j in range(len(ws_uk)):\n",
    "        tdigest_uk.update(ws_uk[j])\n",
    "        tdigest_texas.update(ws_texas[j])\n",
    "\n",
    "    tdigest_list_uk.append(tdigest_uk)\n",
    "    tdigest_list_texas.append(tdigest_texas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are calcluating the difference between the numpy and t-digest estimates for these two specfic locations based on the full range of compression factors. We then conduct a detailed analysis for the 50th and 80th percentile, comparing with the range of estimates obtained from using different numpy interpolation schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific quantiles to examine\n",
    "quantiles = [50, 80]\n",
    "\n",
    "digest_percen_est_uk = np.zeros([num_compressions])\n",
    "digest_percen_est_texas = np.zeros([num_compressions])\n",
    "centriod_num_uk = np.zeros([num_compressions, len(quantiles)])\n",
    "centriod_num_texas = np.zeros([num_compressions, len(quantiles)])\n",
    "\n",
    "errors = np.zeros([2, num_compressions, len(quantiles)])\n",
    "errors_p = np.zeros([2, num_compressions, len(quantiles)])\n",
    "errors_bars = np.zeros([4, num_compressions, len(quantiles)])\n",
    "errors_bars_p = np.zeros([4, num_compressions, len(quantiles)])\n",
    "error_other = np.zeros([8, 2, num_compressions, len(quantiles)])\n",
    "percen_np = np.zeros([8])\n",
    "    \n",
    "for q in range(len(quantiles)):\n",
    "\n",
    "    for i in range(num_compressions):\n",
    "        digest_percen_est_uk[i] = tdigest_list_uk[i].quantile(quantiles[q]/100)\n",
    "        digest_percen_est_texas[i] = tdigest_list_texas[i].quantile(quantiles[q]/100)\n",
    "\n",
    "        centriod_num_uk[i, q] = np.size(tdigest_list_uk[i].centroids())\n",
    "        centriod_num_texas[i, q] = np.size(tdigest_list_texas[i].centroids())\n",
    "\n",
    "    for h in range(2):\n",
    "        if h == 0:\n",
    "            time_series = ws_uk\n",
    "            digest_percen_est = digest_percen_est_uk\n",
    "        elif h == 1:\n",
    "            time_series = ws_texas\n",
    "            digest_percen_est = digest_percen_est_texas\n",
    "\n",
    "        # calculating the numpy percentile estimate using the avaliable interpolation schemes\n",
    "        percen_np[0] = np.percentile(time_series, q = quantiles[q], method = 'inverted_cdf')\n",
    "        percen_np[1] = np.percentile(time_series, q = quantiles[q], method = 'averaged_inverted_cdf')\n",
    "        percen_np[2] = np.percentile(time_series, q = quantiles[q], method = 'closest_observation')\n",
    "        percen_np[3] = np.percentile(time_series, q = quantiles[q], method = 'interpolated_inverted_cdf')\n",
    "        percen_np[4] = np.percentile(time_series, q = quantiles[q], method = 'hazen')\n",
    "        percen_np[5] = np.percentile(time_series, q = quantiles[q], method = 'weibull')\n",
    "        percen_np[6] = np.percentile(time_series, q = quantiles[q], method = 'normal_unbiased')\n",
    "        percen_np[7] = np.percentile(time_series, q = quantiles[q], method = 'linear')\n",
    "\n",
    "        errors[h, :, q] = (digest_percen_est - percen_np[7])\n",
    "        errors_p[h,:, q] = errors[h, :, q]/(percen_np[7]).data * 100 \n",
    "        \n",
    "         # looping through compressions\n",
    "        for kl in range(len(digest_percen_est)):\n",
    "            # absolute error bars\n",
    "            errors_bars[2*h+1, kl, q] = np.max(digest_percen_est[kl] - percen_np)  - errors[h,kl,q]  # upper \n",
    "            errors_bars[2*h, kl, q] = errors[h,kl,q]- np.min(digest_percen_est[kl] - percen_np)  # lower \n",
    "            # percentile error bars\n",
    "            errors_bars_p[2*h+1, kl, q] = (\n",
    "                                (np.max((digest_percen_est[kl] - percen_np)/percen_np))  - \n",
    "                                errors[h,kl,q]/percen_np[7]\n",
    "                                ) * 100 # upper error bar\n",
    "            errors_bars_p[2*h, kl, q] = (\n",
    "                                errors[h,kl,q]/percen_np[7] - np.min((digest_percen_est[kl] - \n",
    "                                percen_np)/percen_np)\n",
    "                                ) * 100 # lower error bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to plot the Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinson = ccrs.Robinson()\n",
    "geodetic = ccrs.Geodetic()\n",
    "fig = plt.figure(constrained_layout=False, figsize=(15,11))\n",
    "fig.subplots_adjust(bottom=0.1, top=0.97, left=0.01, right=0.86, wspace = 0.5, hspace = 0.2)\n",
    "\n",
    "nrow = 2\n",
    "ncol = 2\n",
    "outer = gs.GridSpec(nrow,ncol,wspace=0.2,hspace=0.12, width_ratios=[0.85, 0.8])\n",
    "\n",
    "# fontsize\n",
    "fsize = 14\n",
    "\n",
    "# size of histogram inserts\n",
    "ss_hist = 0.1\n",
    "xp = 0.79\n",
    "\n",
    "# compression factor for the histograms is 60 *array index is 20)\n",
    "comp_index = 20\n",
    "\n",
    "# defining colours for plotting\n",
    "c1 = '#ca0020' \n",
    "c2 = '#f4a582'\n",
    "c3 = '#92c5de' \n",
    "c4 = '#0571b0'\n",
    "\n",
    "# setting the global map\n",
    "# [left, bottom, width, height]\n",
    "ax = fig.add_axes([0.023, 0.62, 0.42, 0.42], projection = robinson)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.05, top=0.99, left=0.01, right=0.9, wspace = 0.2, hspace = 0.4)\n",
    "ax.coastlines(resolution = '10m')\n",
    "\n",
    "#meridians and parallels\n",
    "gl = ax.gridlines(draw_labels = True,linestyle='dashed',alpha=0.25)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlabel_style = {'size': fsize}\n",
    "gl.ylabel_style = {'size': fsize}\n",
    "\n",
    "cmap = plt.get_cmap('Blues', 20)\n",
    "lon2d, lat2d = np.meshgrid(ws.lon, ws.lat)\n",
    "cs = ax.pcolormesh(lon2d, lat2d, total_per_error[0,:,:], transform=ccrs.PlateCarree(),cmap=cmap)\n",
    "\n",
    "lat1 = ws.lat[uk_latlon_index[0]].values\n",
    "lon1 = ws.lon[uk_latlon_index[1]].values\n",
    "\n",
    "lat2 = ws.lat[texas_latlon_index[0]].values\n",
    "lon2 = ws.lon[texas_latlon_index[1]].values\n",
    "\n",
    "#Transform the coordinates of the rectangles to the projection coordinates\n",
    "rect1 = robinson.transform_point(lon1, lat1, geodetic)\n",
    "rect2 = robinson.transform_point(lon2, lat2, geodetic)\n",
    "\n",
    "ax.plot(rect1[0], rect1[1], color=c1, marker='o', markersize=12,\n",
    "        transform=robinson)\n",
    "ax.plot(rect2[0], rect2[1], color=c2, marker='o', markersize=12,\n",
    "    transform=robinson)\n",
    "ax.set_title('(a)', fontsize = fsize, loc = 'left')\n",
    "\n",
    "# Add color bar for each subplot\n",
    "cbar_ax = fig.add_axes([0.06, 0.635, 0.35, 0.01])\n",
    "cbar = fig.colorbar(cs, cax=cbar_ax, orientation='horizontal')\n",
    "#cbar.set_ticks(cbar.get_ticks()[::-1])        \n",
    "cbar.set_label('Mean percentile difference [% of numpy]', rotation=0, fontsize = fsize)\n",
    "cbar.ax.tick_params(labelsize=fsize)\n",
    "cbar.update_ticks()\n",
    "\n",
    "# quantile quantile plot for the UK wind farm\n",
    "percen_digest = np.zeros(100)\n",
    "percen_np = np.zeros(100)\n",
    "quantiles = np.linspace(1, 100, 100)\n",
    "alpha = 0.2\n",
    "# Create main scatter plot\n",
    "\n",
    "ax = plt.subplot(outer[1])\n",
    "ax.set_ylabel(r'T-Digest percentiles - wind speed [m s$^{-1}$]', fontsize = fsize)\n",
    "ax.tick_params('y', labelsize = fsize)\n",
    "ax.tick_params('x', labelsize = fsize)\n",
    "ax.grid(True)\n",
    "ax.set_title('(b)', fontsize = fsize, loc = 'left')\n",
    "\n",
    "# for each percentile extracting the TDigest and numpy estimate\n",
    "for i in range(100):\n",
    "    percen_digest[i] = tdigest_list_uk[comp_index].quantile(quantiles[i]/100)\n",
    "    percen_np[i] = np.percentile(ws_uk, q = quantiles[i])\n",
    "\n",
    "text_height = 0.9*np.max(percen_digest)\n",
    "ax.text(1.2, text_height, 'UK, Moray East (58.25 째N, 2.75 째E)', fontsize = fsize)\n",
    "ax.scatter(percen_np, percen_digest, color=c1)\n",
    "# adding the x = y line \n",
    "x_values = np.linspace(0,np.max(ws_uk),100)\n",
    "y_values = x_values\n",
    "ax.plot(x_values, y_values, label='x = y', linestyle=\"--\", color='black')\n",
    "\n",
    "# adding gray patches to show range of wind farm operating speeds\n",
    "x_start, x_end = 4, 25\n",
    "y_bottom, y_top = 0, 25\n",
    "rectangle = patches.Rectangle((x_start, y_bottom), x_end - x_start, y_top - y_bottom,\n",
    "                            linewidth=1, edgecolor='None', facecolor='gray', \n",
    "                            alpha=alpha)\n",
    "ax.add_patch(rectangle)\n",
    "x_start, x_end = 0, 4\n",
    "y_bottom, y_top = 4, 25\n",
    "rectangle = patches.Rectangle((x_start, y_bottom), x_end - x_start, y_top - y_bottom,\n",
    "                            linewidth=1, edgecolor='None', facecolor='gray', \n",
    "                            alpha=alpha)\n",
    "ax.add_patch(rectangle)  \n",
    "ax.set_xlim([0,np.max(percen_np)+0.5])      \n",
    "ax.set_ylim([0,np.max(percen_digest)+0.5])\n",
    " \n",
    "# adding the histograms\n",
    "bins_space = 30\n",
    "counts, bins = tdigest_list_uk[comp_index].histogram(bins_space)\n",
    "\n",
    "# Create inset axes for the histogram\n",
    "ax_hist2 = fig.add_axes([xp, 0.73, ss_hist, ss_hist])  # [left, bottom, width, height]\n",
    "ax_hist2.hist(bins[:-1], bins=bins, weights=counts, color=c3, edgecolor='black', alpha=1)\n",
    "ax_hist2.set_ylabel('Density', fontsize = fsize)\n",
    "ax_hist2.tick_params('y', labelsize = fsize)\n",
    "ax_hist2.tick_params('x', labelsize = fsize)\n",
    "ax_hist2.grid(True)\n",
    "            \n",
    "#Create inset axes for the histogram\n",
    "ax_hist = fig.add_axes([xp, 0.6, ss_hist, ss_hist])  # [left, bottom, width, height]\n",
    "ax_hist.hist(ws_uk.flatten(), bins=bins, color=c4, edgecolor='black')\n",
    "ax_hist.set_xlabel(r'Wind speed [m s$^{-1}$]', fontsize = fsize)\n",
    "ax_hist.xaxis.set_label_coords(0.4, -0.3)  \n",
    "ax_hist.set_ylabel('Density', fontsize = fsize)\n",
    "ax_hist.tick_params('y', labelsize = fsize)\n",
    "ax_hist.tick_params('x', labelsize = fsize)\n",
    "ax_hist.grid(True)\n",
    "        \n",
    "# quantile quantile plot for the Texas wind farm\n",
    "ax = plt.subplot(outer[3])\n",
    "percen_digest = np.zeros(100)\n",
    "percen_np = np.zeros(100)\n",
    "\n",
    "ax.set_xlabel(r'Numpy percentiles - wind speed [m s$^{-1}$]', fontsize = fsize)        \n",
    "ax.set_ylabel(r'T-Digest percentiles - wind speed [m s$^{-1}$]', fontsize = fsize)\n",
    "ax.tick_params('y', labelsize = fsize)\n",
    "ax.tick_params('x', labelsize = fsize)\n",
    "ax.grid(True)\n",
    "ax.set_xlim([0, np.max(ws_texas)])\n",
    "ax.set_ylim([0, np.max(ws_texas)])\n",
    "ax.set_title('(e)', fontsize = fsize, loc = 'left')\n",
    "\n",
    "# extracting the TDigest and numpy percentile estimates\n",
    "for i in range(100):\n",
    "    percen_digest[i] = tdigest_list_texas[comp_index].quantile(quantiles[i]/100)\n",
    "    percen_np[i] = np.percentile(ws_texas, q = quantiles[i])\n",
    "\n",
    "ax.scatter(percen_np, percen_digest, color=c2)\n",
    "text_height = 0.9*np.max(percen_digest)\n",
    "ax.text(0.7, text_height, 'North America, Roscoe (32.35 째N, 100.45 째W)', fontsize = fsize)\n",
    "\n",
    "# adding x = y line\n",
    "ax.plot(x_values, y_values, label='x = y', linestyle=\"--\", color='black')\n",
    "\n",
    "# adding gray patches to show range of wind farm operating speeds\n",
    "x_start, x_end = 4, 25\n",
    "y_bottom, y_top = 0, 25\n",
    "rectangle = patches.Rectangle((x_start, y_bottom), x_end - x_start, y_top - y_bottom,\n",
    "                            linewidth=1, edgecolor='None', facecolor='gray', \n",
    "                            alpha=alpha)\n",
    "ax.add_patch(rectangle)\n",
    "\n",
    "x_start, x_end = 0, 4\n",
    "y_bottom, y_top = 4, 25\n",
    "rectangle = patches.Rectangle((x_start, y_bottom), x_end - x_start, y_top - y_bottom,\n",
    "                            linewidth=1, edgecolor='None', facecolor='gray', \n",
    "                            alpha=alpha)\n",
    "ax.add_patch(rectangle)\n",
    "\n",
    "counts, bins = tdigest_list_texas[comp_index].histogram(bins_space)\n",
    "\n",
    "# Create inset axes for the histogram\n",
    "ax_hist2 = fig.add_axes([xp, 0.24, ss_hist, ss_hist])  # [left, bottom, width, height]\n",
    "ax_hist2.hist(bins[:-1], bins=bins, weights=counts, color = c3, edgecolor='black', alpha = 1)\n",
    "ax_hist2.set_ylabel('Density', fontsize = fsize)\n",
    "ax_hist2.tick_params('y', labelsize = fsize)\n",
    "ax_hist2.tick_params('x', labelsize = fsize)\n",
    "ax_hist2.grid(True)\n",
    "\n",
    "#Create inset axes for the histogram\n",
    "ax_hist = fig.add_axes([xp, 0.11, ss_hist, ss_hist])  # [left, bottom, width, height]\n",
    "ax_hist.hist(ws_texas.flatten(), bins=bins, color=c4, edgecolor='black')\n",
    "ax_hist.set_xlabel(r'Wind speed [m s$^{-1}$]', fontsize = fsize)\n",
    "ax_hist.xaxis.set_label_coords(0.4, -0.3)  \n",
    "ax_hist.set_ylabel('Density', fontsize = fsize)\n",
    "ax_hist.tick_params('y', labelsize = fsize)\n",
    "ax_hist.tick_params('x', labelsize = fsize)\n",
    "ax_hist.grid(True)\n",
    "       \n",
    "# 50th and 80th percentile errors as a function of compression\n",
    "# factors to get the correct location\n",
    "ss = 0.175\n",
    "extra = 0.005\n",
    "\n",
    "ax = fig.add_axes([0.07, 0.32, ss*2, ss+extra])  # [left, bottom, width, height]\n",
    "ax.errorbar(compressions, errors_p[0, :,0], yerr = errors_bars_p[0:2, :,0], marker = 'o', color=c1,linestyle='')\n",
    "ax.errorbar(compressions, errors_p[1, :,0], yerr = errors_bars_p[2:4, :,0], marker = 'o', color=c2,linestyle='')\n",
    "obh = ax.set_ylabel('Percentile difference [% of numpy]', fontsize = fsize, labelpad = 30)\n",
    "obh.set_position((0, -0.1))  # Adjust the position tuple as needed\n",
    "\n",
    "ax.grid(True)\n",
    "text_height = 0.9*np.max(errors_p[1, :, 0])\n",
    "ax.tick_params('y', labelsize = fsize)\n",
    "ax.tick_params('x', labelsize = fsize)\n",
    "ax.text(100, text_height, r'50$^{\\text{th}}$ percentile', fontsize = fsize)\n",
    "ax.set_title('(c)', fontsize = fsize, loc = 'left')\n",
    "#Create a second x-axis sharing the same x-axis\n",
    "ax2 = ax.twiny()\n",
    "ax2.scatter(centriod_num_uk[:,0], errors[0,:,0], color='gray', marker = '')\n",
    "ax2.set_xlabel('Clusters', color='gray', fontsize = fsize)\n",
    "ax2.tick_params('x', colors='gray', labelsize = fsize)\n",
    "\n",
    "text_height = 0.8*np.max(errors_p[1, :, 1])\n",
    "ax = fig.add_axes([0.07, 0.05, ss*2, ss+extra])  # [left, bottom, width, height]\n",
    "ax.errorbar(compressions, errors_p[0, :, 1], yerr = errors_bars_p[0:2, :,1], marker = 'o', color=c1,linestyle='')\n",
    "ax.errorbar(compressions, errors_p[1, :, 1], yerr = errors_bars_p[2:4, :,1]*2, marker = 'o', color=c2,linestyle='')\n",
    "ax.tick_params('y', labelsize = fsize)\n",
    "ax.tick_params('x', labelsize = fsize)\n",
    "ax.text(100, text_height, r'80$^{\\text{th}}$ percentile', fontsize = fsize)\n",
    "ax.set_xlabel(r'$\\delta$', fontsize = fsize)\n",
    "ax.grid(True)\n",
    "ax.set_title('(d)', fontsize = fsize, loc = 'left')\n",
    "ax2 = ax.twiny()\n",
    "ax2.scatter(centriod_num_uk[:,0], errors[0,:,0], color='gray', marker = '')\n",
    "ax2.tick_params('x', colors='gray', labelsize = fsize)\n",
    "                \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
